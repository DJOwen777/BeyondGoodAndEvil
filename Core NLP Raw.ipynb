{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import stanfordnlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from os import getcwd\n",
    "from os import environ\n",
    "from random import sample \n",
    "\n",
    "import Levenshtein as lev\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "#set environment variable\n",
    "cwd = getcwd()\n",
    "environ['CORENLP_HOME'] = cwd + '\\\\corenlp\\\\stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this import is seperate in case you are running the client without using a gpu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom functions for convenience and saving lines of code\n",
    "\n",
    "#add final results of algorithm to results dataframe\n",
    "def addResults(results, rlabel, rmessageOG, rcleanMessage, rWarning, rRatio, rClass): \n",
    "    results.loc[-1] = [rlabel, rmessageOG, rcleanMessage, rWarning, rRatio, rClass]\n",
    "    results.index = results.index+1\n",
    "    results.reindex(index=results.index[::-1])\n",
    "\n",
    "#add detected taged words\n",
    "def addTags(tags, tsentence, ttoken, tvalue, toriginal): \n",
    "    tags.loc[-1] = [tsentence, ttoken, tvalue, toriginal]\n",
    "    tags.index = tags.index+1\n",
    "    tags.reindex(index=tags.index[::-1])\n",
    "\n",
    "#add detected dependencies from detected words \n",
    "def addFlags(flags, fsentence, fedge, fsources, fsource_words, ftargets, ftarget_words, fdependencies, forigin):\n",
    "    #for forigin the opposite of what triggered the detection is what will be placed there for amplifier detection \n",
    "    #convenience later (for example if the target is nigger, the forigin will be 'source words' to check for amplifiers)\n",
    "    flags.loc[-1] = [fsentence, fedge, fsources, fsource_words, ftargets, ftarget_words, fdependencies, forigin]\n",
    "    flags.index = flags.index + 1\n",
    "    flags = flags.sort_index(inplace=True)\n",
    "\n",
    "#add info for detected words from Hatebase data\n",
    "def addInfo(termInfo, ambiguousHateBaseDB, unambiguousHateBaseDB, amOffAverage, unOffAverage, element):    \n",
    "    termInfo = termInfo.append(ambiguousHateBaseDB.loc[ambiguousHateBaseDB['term'].str.lower()\\\n",
    "                                                       == element.lower()], ignore_index=True) \n",
    "    termInfo['average_offensiveness'].fillna(amOffAverage, inplace=True)\n",
    "    termInfo = termInfo.append(unambiguousHateBaseDB.loc[unambiguousHateBaseDB['term'].str.lower()\\\n",
    "                                                       == element.lower()], ignore_index=True) \n",
    "    termInfo['average_offensiveness'].fillna(unOffAverage, inplace=True)\n",
    "    termInfo['hateful_meaning'] = termInfo['hateful_meaning'].str.lower() \n",
    "    return termInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Hatebase data, divided on ambiguity \n",
    "ambiguousHateBaseDB = pd.read_csv(\"total_ambiguous_results.csv\", index_col=False)\n",
    "unambiguousHateBaseDB = pd.read_csv('total_unambiguous_results.csv', index_col=False)\n",
    "#read in amplifiers list\n",
    "amplifiersDB = pd.read_csv('noswearing_trim_data.csv', index_col=False) \n",
    "\n",
    "#form lists from above data for convenience \n",
    "amplifiersList = amplifiersDB['term']\n",
    "ambiguousHateTermsList = ambiguousHateBaseDB['term']\n",
    "unambiguousHateTermsList = unambiguousHateBaseDB['term']\n",
    "\n",
    "#calcualte averages from Hatebase lists\n",
    "unOffAverage = unambiguousHateBaseDB[\"average_offensiveness\"].mean()\n",
    "amOffAverage = ambiguousHateBaseDB['average_offensiveness'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in tweet data\n",
    "fullMessagesDB = pd.read_csv('davidson_labeled_data_replaced.csv')\n",
    "hateMessagesDB = pd.read_csv(\"DavidsonHateOnly.csv\", index_col=False)\n",
    "cleanMessagesDB = pd.read_csv(\"DavidsonCleanOnly.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to store final results of client\n",
    "results = pd.DataFrame(columns = ['label', 'message', 'clean', 'warning', 'hate_ratio', 'classOG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDB = fullMessagesDB #used so you only need to edit here to change tweet db being used\n",
    "\n",
    "#set global variables\n",
    "cutoff = .95 #jaro similarity cutoff\n",
    "offCutoff = 90 #offensive cutoff, 75\n",
    "offCutoff2 = 270 #second offensive cutoff, 225\n",
    "\n",
    "exDeps = ['punct', 'compound'] #dependencies which can be ignored as they are unhelpful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=50000, memory='24G') as client: \n",
    "    for m in range (0, currentDB.shape[0]): #for each tweet in currentDB: \n",
    "        messageOG =  currentDB.loc[m, 'tweet'] #select current tweet \n",
    "        messageVoteRatio = currentDB.loc[m,'hate_speech'] / currentDB.loc[m,'count'] #retrieve original vote ratio of hate vs total\n",
    "        messageClass = currentDB.loc[m, 'class'] #retrieve original class given\n",
    "        \n",
    "        print('\\nOriginal Message: ', messageOG, '\\n')\n",
    "        document = client.annotate(messageOG) #run nlp on current tweet\n",
    "        sentences = document.sentence #shortened here for convenience\n",
    "        \n",
    "        \n",
    "        #create dataframes to be used for analysis of current tweet\n",
    "        tags = pd.DataFrame(columns = ['sentence', 'token', 'term', 'original'])\n",
    "        tTags = pd.DataFrame(columns = ['sentence', 'token', 'term', 'original'])\n",
    "        termInfo = pd.DataFrame(columns = ambiguousHateBaseDB.columns.values.tolist()) \n",
    "        tTermInfo = pd.DataFrame(columns = ambiguousHateBaseDB.columns.values.tolist()) \n",
    "        flags = pd.DataFrame(columns = ['sentence', 'edge', 'source', 'source words', 'target', 'target words',\\\n",
    "                                        'dependencies', 'modifier'])\n",
    "        \n",
    "        #create variables to be used for analysis of current tweet\n",
    "        tempTwoList = []\n",
    "        splitTermList = []\n",
    "        messageArray = messageOG.split()\n",
    "        cleanMessage = ''\n",
    "        label = 0 #0 = clean, 1 = warning, 2 = block certain words, 3 = block entire message\n",
    "        loopBool = False \n",
    "        offScore = 0\n",
    "        ampScore = 0\n",
    "        \n",
    "        \n",
    "        #add tags for two word terms\n",
    "        for a in range(0, len(messageArray)-1): \n",
    "            tString = ''\n",
    "            tString = messageArray[a] + ' ' + messageArray[a+1]\n",
    "            for element in unambiguousHateTermsList: \n",
    "                if lev.jaro(tString.lower(), element.lower()) >= cutoff: \n",
    "                    label = label + 2\n",
    "                    tempTwoList.append(tString.lower().split())\n",
    "                    termInfo = addInfo(termInfo, ambiguousHateBaseDB, unambiguousHateBaseDB, amOffAverage, unOffAverage, element.lower()) \n",
    "            for element in ambiguousHateTermsList: \n",
    "                if lev.jaro(tString.lower(), element.lower()) >= cutoff: \n",
    "                    tempTwoList.append(tString.lower().split())\n",
    "                    termInfo = addInfo(termInfo, ambiguousHateBaseDB, unambiguousHateBaseDB, amOffAverage, unOffAverage, element.lower()) \n",
    "        \n",
    "        for element in termInfo['term']: \n",
    "            splitTermList.append(element.split())\n",
    "        \n",
    "        \n",
    "        #add tags for single word terms\n",
    "        for s in range (0, len(sentences)): \n",
    "            for t in range(0, len(sentences[s].token)): \n",
    "                if any(sentences[s].token[t].value.lower() in sublist for sublist in tempTwoList): \n",
    "                    addTags(tags, s, t, sentences[s].token[t].value.lower(), sentences[s].token[t].value)\n",
    "                else: \n",
    "                    loopBool = False\n",
    "                    for element in unambiguousHateTermsList: #ambiguous originally\n",
    "                        if lev.jaro(sentences[s].token[t].value.lower(), element.lower()) >= cutoff and loopBool == False: \n",
    "                            label = label + 2\n",
    "                            addTags(tags, s, t, element.lower(), sentences[s].token[t].value)\n",
    "                            loopBool = True #used to prevent needlessly going through the rest of the list if a match is found\n",
    "                    for element in ambiguousHateTermsList: #unambiguous originally\n",
    "                        if lev.jaro(sentences[s].token[t].value.lower(), element.lower()) >= cutoff and loopBool == False: \n",
    "                            addTags(tags, s, t, element.lower(), sentences[s].token[t].value)\n",
    "                            loopBool = True\n",
    "                    for element in amplifiersList: \n",
    "                        if lev.jaro(sentences[s].token[t].value.lower(), element.lower()) >= cutoff and loopBool == False: \n",
    "                            addTags(tags, s, t, element.lower(), sentences[s].token[t].value)\n",
    "                            loopBool = True\n",
    "                            \n",
    "                            \n",
    "        #detect dependencies for tagged words\n",
    "        if np.isnan(tags['sentence'].max()): #if no tags were detected\n",
    "            tsLength = 0\n",
    "        else: \n",
    "            tsLength = int(tags['sentence'].max()+1)\n",
    "        for s in range(0, tsLength): #for each sentence up to the last one a tag was detected\n",
    "            tTags = tags.loc[(tags['sentence'] == s)] #select all tags for current sentence\n",
    "            for e in range(0, len(sentences[s].basicDependencies.edge)): #for each edge generated for current sentence \n",
    "                current = sentences[s].basicDependencies.edge[e] #created for abbreviation \n",
    "                if current.dep not in exDeps: #if the dependency detected is not one of the ones we choose to ignore (punct, etc)\n",
    "                    if current.source-1 in set(tTags['token']): #if the dependencies source token has been tagged\n",
    "                        tTarget = sentences[s].token[current.target-1].value #created for abbreviation\n",
    "                        if tTarget in set(tTags['term']): #if dependency target is a tagged word\n",
    "                            tTarget = tTags.loc[(tTags['token'] == current.target-1)]['term'].iloc[0] #created for abbreviation\n",
    "                        addFlags(flags, s, e, current.source, tTags.loc[(tTags['token'] == current.source-1)]['term'].iloc[0], current.target, \\\n",
    "                                 tTarget, current.dep, 'target words') #add data from dependency to flags\n",
    "                    elif current.target-1 in set(tTags['token']): #if the dependencies target token has been tagged \n",
    "                        tSource = sentences[s].token[current.source-1].value #created for abbreviation\n",
    "                        if tSource in set(tTags['term']): #if dependency source is a tagged word\n",
    "                            tSource = tTags.loc[(tTags['token'] == current.source-1)]['term'].iloc[0] #created for abbreviation\n",
    "                        addFlags(flags, s, e, current.source, tSource, current.target, \\\n",
    "                                 tTags.loc[(tTags['token'] == current.target-1)]['term'].iloc[0], current.dep, \\\n",
    "                                 'source words') #add data from dependency to flags\n",
    "                        \n",
    "                        \n",
    "        #retrieve full data on file for problem words \n",
    "        for index, row in tags.iterrows(): \n",
    "            #check for no duplicates of two word terms\n",
    "            if not any(row['term'] in sublist for sublist in splitTermList): \n",
    "                #add info from HateBase DB to termInfo\n",
    "                termInfo = addInfo(termInfo, ambiguousHateBaseDB, unambiguousHateBaseDB, amOffAverage, unOffAverage, row['term']) \n",
    "                \n",
    "                \n",
    "        #check and sum offensiveness for terms (for those that have one)\n",
    "        offScore = np.nansum(termInfo.loc[termInfo['is_unambiguous'] == False]['average_offensiveness'])\n",
    "        #the False unambiguous is to make sure we're not double counting unambiguous terms when they already auto get their label boosted. \n",
    "        \n",
    "                    \n",
    "        for index, row in flags.iterrows(): #for each flag\n",
    "            for element in amplifiersList: #for each element in amplifiers\n",
    "                if lev.jaro(row[row['modifier']], element.lower()) >= cutoff: \n",
    "                    if row['modifier'] == 'source words' and row['target words'] in termInfo['term']: \n",
    "                        offScore += termInfo.loc[termInfo['term'] == str(row['target words'])].loc[0,'average_offensiveness']\n",
    "                    elif row['modifier'] == 'target words' and row['source words'] in termInfo['term']:\n",
    "                        offScore += termInfo.loc[termInfo['term'] == str(row['source words'])].loc[0,'average_offensiveness']\n",
    "        \n",
    "        if offScore >= offCutoff2: \n",
    "            label += 2\n",
    "        elif offScore >= offCutoff: \n",
    "            label += 1\n",
    "                    \n",
    "                    \n",
    "        #begin creating a list of descriptions for offending words   \n",
    "        meaningList = termInfo['hateful_meaning'].tolist()\n",
    "        meaningList = [x.lower() for x in meaningList if str(x) != 'nan'] \n",
    "        meaningModify = []\n",
    "\n",
    "        for m in meaningList: \n",
    "            #this is to trim out descrptions with multiple sections in the form of [1]...[2]...etc and only select the first part of the description \n",
    "            if ']' in m: \n",
    "                temp = m.split(']')[1].split('[')[0].lstrip().rstrip()\n",
    "                meaningModify.append(temp.split('.')[0].replace('.', '').split(',')[0])\n",
    "            else: \n",
    "                meaningModify.append(m.lstrip().rstrip().split('.')[0].replace('.', '').split(',')[0])\n",
    "                \n",
    "             \n",
    "        #create a description for offensive words that are not given one in the Hatebase DB\n",
    "        aboutIndex = []\n",
    "        aboutArray = ['nationality', 'ethnicity', 'religion', 'gender', 'sexual orientation', 'disability', 'class']\n",
    "\n",
    "        for index, row in termInfo.iterrows(): \n",
    "            thateful = str(row['hateful_meaning'])\n",
    "            if thateful == 'nan' or thateful == '': \n",
    "                tTermInfo = tTermInfo.append(termInfo.loc[index,:])\n",
    "        \n",
    "        if not tTermInfo.empty: \n",
    "            if True in set(tTermInfo['is_about_nationality']): aboutIndex.append(0) \n",
    "            if True in set(tTermInfo['is_about_ethnicity']): aboutIndex.append(1) \n",
    "            if True in set(tTermInfo['is_about_religion']): aboutIndex.append(2)\n",
    "            if True in set(tTermInfo['is_about_gender']): aboutIndex.append(3)\n",
    "            if True in set(tTermInfo['is_about_sexual_orientation']): aboutIndex.append(4)\n",
    "            if True in set(tTermInfo['is_about_disability']): aboutIndex.append(5) \n",
    "            if True in set(tTermInfo['is_about_class']): aboutIndex.append(6) \n",
    "\n",
    "        for a in range(0, len(aboutIndex)): \n",
    "            meaningModify.append('a person\\'s ' + aboutArray[aboutIndex[a]])\n",
    "            \n",
    "            \n",
    "        #change to a set to eliminate any possible duplicate descriptions\n",
    "        meaningModifyOG = meaningModify #maintain all original meanings for replacement later\n",
    "        meaningModify = set(meaningModify)\n",
    "        meaningModify = list(meaningModify)\n",
    "        \n",
    "        \n",
    "        #join the descriptions in a readable manner. \n",
    "        meaningString = \" and \".join([\", \".join(meaningModify[:-1]), meaningModify[-1]] if len(meaningModify) > 2 else meaningModify) \n",
    "        \n",
    "        meaningString = meaningString + '.' #add a period to the end of the description. \n",
    "        \n",
    "        \n",
    "        if label == 0: \n",
    "            print('This message is clean of hate speech.')\n",
    "            rwarning = 'This message is clean of hate speech.'\n",
    "            addResults(results, label, messageOG, cleanMessage, rwarning, messageVoteRatio, messageClass)\n",
    "            print(messageOG)\n",
    "            \n",
    "            \n",
    "        if label == 1: \n",
    "            print('WARNING: This message may contain some hate speech about', meaningString, \"\\n\") \n",
    "            rwarning = 'WARNING: This message may contain some hate speech about '+meaningString\n",
    "            addResults(results, label, messageOG, cleanMessage, rwarning, messageVoteRatio, messageClass)\n",
    "            print(messageOG)\n",
    "            \n",
    "            \n",
    "        if label == 2: \n",
    "            print('WARNING: Parts of this message have been censored due to hate speech about', meaningString) \n",
    "            cleanArray = []\n",
    "            count = 0\n",
    "            cleanBool = False\n",
    "            \n",
    "            #run through message checking for offending words and replacing them with their meanings\n",
    "            for e in range(0, len(messageArray)):                 \n",
    "                cleanBool = False\n",
    "                #check for single word tags\n",
    "                cleanE = messageArray[e].replace('.', '').replace('\\\"', '')\n",
    "                for index, row in tags.iterrows(): \n",
    "                    if cleanE == row['original'] and row['term'] in set(termInfo['term']) and cleanBool == False:  \n",
    "                        tmeaning = meaningModifyOG[count]\n",
    "                        if tmeaning [0:2] == 'a ': \n",
    "                            tmeaning = tmeaning[2:]\n",
    "                        cleanArray.append(\"\\\"\" + tmeaning + \"\\\"\")\n",
    "                        count = count+1\n",
    "                        cleanBool = True\n",
    "                #check for two word terms from termInfo\n",
    "                if e < len(messageArray)-2 and cleanBool == False: \n",
    "                    tString = messageArray[e] + ' ' + messageArray[e+1]\n",
    "                    if tString.replace('.', '').replace('\\\"', '') in set(termInfo['term']): \n",
    "                        tmeaning = meaningModifyOG[count]\n",
    "                        if tmeaning [0:2] == 'a ': \n",
    "                            tmeaning = tmeaning[2:]\n",
    "                        cleanArray.append(\"\\\"\" + meaningModifyOG[count] + \"\\\"\")\n",
    "                        count = count+1\n",
    "                    else: \n",
    "                        cleanArray.append(messageArray[e])\n",
    "                #if this part of messageArray isn't tagged add the original message part\n",
    "                elif cleanBool == False: \n",
    "                    cleanArray.append(messageArray[e])\n",
    "            \n",
    "            #add spaces to cleanMessage created above to make it readable\n",
    "            cleanMessage = ' '.join([str(elem) for elem in cleanArray]) + '.'\n",
    "            \n",
    "            rwarning = 'WARNING: Parts of this message have been censored due to hate speech.'\n",
    "            addResults(results, label, messageOG, cleanMessage, rwarning, messageVoteRatio, messageClass)\n",
    "            print(cleanMessage) \n",
    "            \n",
    "            \n",
    "        if label >= 3: \n",
    "            print('This message has been blocked due to hate speech about', meaningString) \n",
    "            rwarning = 'This message has been blocked due to hate speech about '+ meaningString\n",
    "            addResults(results, label, messageOG, cleanMessage, rwarning, messageVoteRatio, messageClass)\n",
    "            \n",
    "            \n",
    "        print('Client Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.classOG.value_counts()\n",
    "#Class 0 = Hate Speech, Class 1 = Offensive Language, Class 2 = Neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.hate_ratio.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(results.label,results.hate_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('full_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
