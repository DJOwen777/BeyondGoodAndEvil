{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import stanfordnlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import re\n",
    "\n",
    "import Levenshtein as lev\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "from os import getcwd\n",
    "from os import environ\n",
    "\n",
    "#set environment variable\n",
    "cwd = getcwd()\n",
    "environ['CORENLP_HOME'] = cwd + '\\\\corenlp\\\\stanford-corenlp-full-2018-10-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this import is seperate in case you are running the client without using a gpu\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom functions for convenience and saving lines of code\n",
    "\n",
    "#add final results of algorithm to results dataframe\n",
    "def add_results(results, rlabel, rmessage_OG, rclean_message, rWarning, rRatio, rClass): \n",
    "    results.loc[-1] = [rlabel, rmessage_OG, rclean_message, rWarning, rRatio, rClass]\n",
    "    results.index = results.index+1\n",
    "    results.reindex(index=results.index[::-1])\n",
    "\n",
    "#add detected taged words\n",
    "def add_tags(tags, tsentence, ttoken, tvalue, toriginal): \n",
    "    tags.loc[-1] = [tsentence, ttoken, tvalue, toriginal]\n",
    "    tags.index = tags.index+1\n",
    "    tags.reindex(index=tags.index[::-1])\n",
    "\n",
    "#add detected dependencies from detected words \n",
    "def add_flags(flags, fsentence, fedge, fsources, fsource_words, ftargets, ftarget_words, fdependencies, forigin):\n",
    "    #for forigin the opposite of what triggered the detection is what will be placed there for amplifier detection \n",
    "    #convenience later (for example if the target is nigger, the forigin will be 'source words' to check for amplifiers)\n",
    "    flags.loc[-1] = [fsentence, fedge, fsources, fsource_words, ftargets, ftarget_words, fdependencies, forigin]\n",
    "    flags.index = flags.index + 1\n",
    "    \n",
    "    #flags = flags.sort_index(inplace=True)\n",
    "    flags.reindex(index=flags.index[::-1])\n",
    "\n",
    "#add info for detected words from Hatebase data\n",
    "def add_info(term_info, ambiguous_hate_base_DB, unambiguous_hate_base_DB, am_off_average, un_off_average, element):    \n",
    "    term_info = term_info.append(ambiguous_hate_base_DB.loc[ambiguous_hate_base_DB['term'].str.lower()\\\n",
    "                                                       == element.lower()], ignore_index=True) \n",
    "    term_info['average_offensiveness'].fillna(am_off_average, inplace=True)\n",
    "    term_info = term_info.append(unambiguous_hate_base_DB.loc[unambiguous_hate_base_DB['term'].str.lower()\\\n",
    "                                                       == element.lower()], ignore_index=True) \n",
    "    term_info['average_offensiveness'].fillna(un_off_average, inplace=True)\n",
    "    term_info['hateful_meaning'] = term_info['hateful_meaning'].str.lower() \n",
    "    return term_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Hatebase data, divided on ambiguity \n",
    "ambiguous_hate_base_DB = pd.read_csv(\"total_ambiguous_results.csv\", index_col=False)\n",
    "unambiguous_hate_base_DB = pd.read_csv('total_unambiguous_results.csv', index_col=False)\n",
    "#read in amplifiers list\n",
    "amplifiers_DB = pd.read_csv('noswearing_trim_data.csv', index_col=False) \n",
    "\n",
    "#form lists from above data for convenience \n",
    "amplifiers_list = amplifiers_DB['term']\n",
    "ambiguous_hate_terms_list = ambiguous_hate_base_DB['term']\n",
    "unambiguous_hate_terms_list = unambiguous_hate_base_DB['term']\n",
    "\n",
    "#calcualte averages from Hatebase lists\n",
    "un_off_average = unambiguous_hate_base_DB[\"average_offensiveness\"].mean()\n",
    "am_off_average = ambiguous_hate_base_DB['average_offensiveness'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in tweet data\n",
    "full_messages_DB = pd.read_csv('davidson_labeled_data_replaced.csv')\n",
    "hate_messages_DB = pd.read_csv(\"davidson_hate_only.csv\", index_col=False)\n",
    "clean_messages_DB = pd.read_csv(\"davidson_clean_only.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe to store final results of client\n",
    "results = pd.DataFrame(columns = ['label', 'message', 'clean', 'warning', 'hate_ratio', 'classOG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_DB = full_messages_DB #used so you only need to edit here to change tweet db being used\n",
    "\n",
    "#set global variables\n",
    "cutoff = .95 #jaro similarity cutoff\n",
    "off_cutoff = 90 #offensive cutoff\n",
    "off_cutoff_2 = 270 #second offensive cutoff\n",
    "\n",
    "ex_deps = ['punct', 'compound'] #dependencies which can be ignored as they are unhelpful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CoreNLPClient(annotators=['tokenize','ssplit','lemma', 'pos', 'depparse'], timeout=50000, memory='24G') as client: \n",
    "    #previous annotators: parse, ner, coref\n",
    "    for m in range (0, current_DB.shape[0]): #for each tweet in current_DB: \n",
    "        message_OG =  current_DB.loc[m, 'tweet'] #select current tweet \n",
    "        message_vote_ratio = current_DB.loc[m,'hate_speech'] / current_DB.loc[m,'count'] #retrieve original vote ratio of hate vs total\n",
    "        message_class = current_DB.loc[m, 'class'] #retrieve original class given\n",
    "        \n",
    "        print('\\nOriginal Message: ', message_OG, '\\n')\n",
    "        document = client.annotate(message_OG) #run nlp on current tweet\n",
    "        sentences = document.sentence #shortened here for convenience\n",
    "        \n",
    "        \n",
    "        #create dataframes to be used for analysis of current tweet\n",
    "        tags = pd.DataFrame(columns = ['sentence', 'token', 'term', 'original'])\n",
    "        t_tags = pd.DataFrame(columns = tags.columns.values.tolist())\n",
    "        term_info = pd.DataFrame(columns = ambiguous_hate_base_DB.columns.values.tolist()) \n",
    "        t_term_info = pd.DataFrame(columns = ambiguous_hate_base_DB.columns.values.tolist()) \n",
    "        flags = pd.DataFrame(columns = ['sentence', 'edge', 'source', 'source words', 'target', 'target words',\\\n",
    "                                        'dependencies', 'modifier'])\n",
    "        \n",
    "        #create variables to be used for analysis of current tweet\n",
    "        temp_two_list, split_term_list = [], []\n",
    "        message_array = message_OG.split()\n",
    "        clean_message = ''\n",
    "        loop_bool = False \n",
    "        label = 0 #0 = clean, 1 = warning, 2 = block certain words, 3 = block entire message\n",
    "        off_score = 0\n",
    "        \n",
    "        \n",
    "        #add tags for two word terms\n",
    "        for a in range(0, len(message_array)-1): \n",
    "            t_string = message_array[a] + ' ' + message_array[a+1]\n",
    "            for element in unambiguous_hate_terms_list: \n",
    "                if lev.jaro(t_string.lower(), element.lower()) >= cutoff: \n",
    "                    label = label + 2\n",
    "                    temp_two_list.append(t_string.lower().split())\n",
    "                    term_info = add_info(term_info, ambiguous_hate_base_DB, unambiguous_hate_base_DB, am_off_average, un_off_average, element.lower()) \n",
    "            for element in ambiguous_hate_terms_list: \n",
    "                if lev.jaro(t_string.lower(), element.lower()) >= cutoff: \n",
    "                    temp_two_list.append(t_string.lower().split())\n",
    "                    term_info = add_info(term_info, ambiguous_hate_base_DB, unambiguous_hate_base_DB, am_off_average, un_off_average, element.lower()) \n",
    "        \n",
    "        for element in term_info['term']: \n",
    "            split_term_list.append(element.split())\n",
    "        \n",
    "        \n",
    "        #add tags for single word terms\n",
    "        for s in range (0, len(sentences)): \n",
    "            for t in range(0, len(sentences[s].token)): \n",
    "                if any(sentences[s].token[t].value.lower() in sublist for sublist in temp_two_list): \n",
    "                    add_tags(tags, s, t, sentences[s].token[t].value.lower(), sentences[s].token[t].value)\n",
    "                else: \n",
    "                    loop_bool = False\n",
    "                    for element in unambiguous_hate_terms_list: #ambiguous originally\n",
    "                        if lev.jaro(sentences[s].token[t].value.lower(), element.lower()) >= cutoff and loop_bool == False: \n",
    "                            label = label + 2\n",
    "                            add_tags(tags, s, t, element.lower(), sentences[s].token[t].value)\n",
    "                            loop_bool = True #used to prevent needlessly going through the rest of the list if a match is found\n",
    "                    for element in ambiguous_hate_terms_list: #unambiguous originally\n",
    "                        if lev.jaro(sentences[s].token[t].value.lower(), element.lower()) >= cutoff and loop_bool == False: \n",
    "                            add_tags(tags, s, t, element.lower(), sentences[s].token[t].value)\n",
    "                            loop_bool = True\n",
    "                    for element in amplifiers_list: \n",
    "                        if lev.jaro(sentences[s].token[t].value.lower(), element.lower()) >= cutoff and loop_bool == False: \n",
    "                            add_tags(tags, s, t, element.lower(), sentences[s].token[t].value)\n",
    "                            loop_bool = True\n",
    "                            \n",
    "                            \n",
    "        #detect dependencies for tagged words\n",
    "        if np.isnan(tags['sentence'].max()): #if no tags were detected\n",
    "            ts_length = 0\n",
    "        else: \n",
    "            ts_length = int(tags['sentence'].max()+1)\n",
    "        for s in range(0, ts_length): #for each sentence up to the last one a tag was detected\n",
    "            t_tags = tags.loc[(tags['sentence'] == s)] #select all tags for current sentence\n",
    "            for e in range(0, len(sentences[s].basicDependencies.edge)): #for each edge generated for current sentence \n",
    "                current = sentences[s].basicDependencies.edge[e] #created for abbreviation \n",
    "                if current.dep not in ex_deps: #if the dependency detected is not one of the ones we choose to ignore (punct, etc)\n",
    "                    if current.source-1 in set(t_tags['token']): #if the dependencies source token has been tagged\n",
    "                        tTarget = sentences[s].token[current.target-1].value #created for abbreviation\n",
    "                        if tTarget in set(t_tags['term']): #if dependency target is a tagged word\n",
    "                            tTarget = t_tags.loc[(t_tags['token'] == current.target-1)]['term'].iloc[0] #created for abbreviation\n",
    "                        add_flags(flags, s, e, current.source, t_tags.loc[(t_tags['token'] == current.source-1)]['term'].iloc[0], current.target, \\\n",
    "                                 tTarget, current.dep, 'target words') #add data from dependency to flags\n",
    "                    elif current.target-1 in set(t_tags['token']): #if the dependencies target token has been tagged \n",
    "                        t_source = sentences[s].token[current.source-1].value #created for abbreviation\n",
    "                        if t_source in set(t_tags['term']): #if dependency source is a tagged word\n",
    "                            t_source = t_tags.loc[(t_tags['token'] == current.source-1)]['term'].iloc[0] #created for abbreviation\n",
    "                        add_flags(flags, s, e, current.source, t_source, current.target, \\\n",
    "                                 t_tags.loc[(t_tags['token'] == current.target-1)]['term'].iloc[0], current.dep, \\\n",
    "                                 'source words') #add data from dependency to flags\n",
    "                        \n",
    "                        \n",
    "        #retrieve full data on file for problem words \n",
    "        for index, row in tags.iterrows(): \n",
    "            #check for no duplicates of two word terms\n",
    "            if not any(row['term'] in sublist for sublist in split_term_list): \n",
    "                #add info from HateBase DB to term_info\n",
    "                term_info = add_info(term_info, ambiguous_hate_base_DB, unambiguous_hate_base_DB, am_off_average, un_off_average, row['term']) \n",
    "                \n",
    "                \n",
    "        #check and sum offensiveness for terms (for those that have one)\n",
    "        off_score = np.nansum(term_info.loc[term_info['is_unambiguous'] == False]['average_offensiveness'])\n",
    "        #the False unambiguous is to make sure we're not double counting unambiguous terms when they already auto get their label boosted. \n",
    "        \n",
    "                    \n",
    "        for index, row in flags.iterrows(): #for each flag\n",
    "            for element in amplifiers_list: #for each element in amplifiers\n",
    "                if lev.jaro(row[row['modifier']], element.lower()) >= cutoff: \n",
    "                    if row['modifier'] == 'source words' and row['target words'] in term_info['term']: \n",
    "                        off_score += term_info.loc[term_info['term'] == str(row['target words'])].loc[0,'average_offensiveness']\n",
    "                    elif row['modifier'] == 'target words' and row['source words'] in term_info['term']:\n",
    "                        off_score += term_info.loc[term_info['term'] == str(row['source words'])].loc[0,'average_offensiveness']\n",
    "        \n",
    "        if off_score >= off_cutoff_2: \n",
    "            label += 2\n",
    "        elif off_score >= off_cutoff: \n",
    "            label += 1\n",
    "                    \n",
    "                    \n",
    "        #begin creating a list of descriptions for offending words   \n",
    "        meaning_list = term_info['hateful_meaning'].tolist()\n",
    "        meaning_list = [x.lower() for x in meaning_list if str(x) != 'nan'] \n",
    "        meaning_modify = []\n",
    "\n",
    "        for m in meaning_list: \n",
    "            #this is to trim out descrptions with multiple sections in the form of [1]...[2]...etc and only select the first part of the description \n",
    "            if ']' in m: \n",
    "                temp = m.split(']')[1].split('[')[0].lstrip().rstrip()\n",
    "                meaning_modify.append(temp.split('.')[0].replace('.', '').split(',')[0])\n",
    "            else: \n",
    "                meaning_modify.append(m.lstrip().rstrip().split('.')[0].replace('.', '').split(',')[0])\n",
    "                \n",
    "             \n",
    "        #create a description for offensive words that are not given one in the Hatebase DB\n",
    "        about_index = []\n",
    "        about_array = ['nationality', 'ethnicity', 'religion', 'gender', 'sexual orientation', 'disability', 'class']\n",
    "\n",
    "        for index, row in term_info.iterrows(): \n",
    "            thateful = str(row['hateful_meaning'])\n",
    "            if thateful == 'nan' or thateful == '': \n",
    "                t_term_info = t_term_info.append(term_info.loc[index,:])\n",
    "        \n",
    "        if not t_term_info.empty: \n",
    "            if True in set(t_term_info['is_about_nationality']): about_index.append(0) \n",
    "            if True in set(t_term_info['is_about_ethnicity']): about_index.append(1) \n",
    "            if True in set(t_term_info['is_about_religion']): about_index.append(2)\n",
    "            if True in set(t_term_info['is_about_gender']): about_index.append(3)\n",
    "            if True in set(t_term_info['is_about_sexual_orientation']): about_index.append(4)\n",
    "            if True in set(t_term_info['is_about_disability']): about_index.append(5) \n",
    "            if True in set(t_term_info['is_about_class']): about_index.append(6) \n",
    "\n",
    "        for a in range(0, len(about_index)): \n",
    "            meaning_modify.append('a person\\'s ' + about_array[about_index[a]])\n",
    "            \n",
    "            \n",
    "        #change to a set to eliminate any possible duplicate descriptions\n",
    "        meaning_modify_OG = meaning_modify.copy() #maintain all original meanings for replacement later\n",
    "        meaning_modify = set(meaning_modify)\n",
    "        meaning_modify = list(meaning_modify)\n",
    "        \n",
    "        \n",
    "        #join the descriptions in a readable manner. \n",
    "        meaning_string = \" and \".join([\", \".join(meaning_modify[:-1]), meaning_modify[-1]] if len(meaning_modify) > 2 else meaning_modify) \n",
    "        meaning_string = meaning_string + '.' #add a period to the end of the description. \n",
    "        \n",
    "        \n",
    "        if label == 0: \n",
    "            print('This message is clean of hate speech.')\n",
    "            rwarning = 'This message is clean of hate speech.'\n",
    "            add_results(results, label, message_OG, clean_message, rwarning, message_vote_ratio, message_class)\n",
    "            print(message_OG)\n",
    "            \n",
    "            \n",
    "        if label == 1: \n",
    "            print('WARNING: This message may contain some hate speech about ', meaning_string, \"\\n\") \n",
    "            rwarning = 'WARNING: This message may contain some hate speech about '+ meaning_string\n",
    "            add_results(results, label, message_OG, clean_message, rwarning, message_vote_ratio, message_class)\n",
    "            print(message_OG)\n",
    "            \n",
    "            \n",
    "        if label == 2: \n",
    "            print('WARNING: Parts of this message have been censored due to hate speech about', meaning_string) \n",
    "            \n",
    "            clean_array, count, clean_bool = [], 0, False\n",
    "            \n",
    "            #run through message checking for offending words and replacing them with their meanings\n",
    "            for e in range(0, len(message_array)):                 \n",
    "                clean_bool = False\n",
    "                #check for single word tags\n",
    "                cleanE = message_array[e].replace('.', '').replace('\\\"', '')\n",
    "                for index, row in tags.iterrows(): \n",
    "                    if cleanE == row['original'] and row['term'] in set(term_info['term']) and clean_bool == False:  \n",
    "                        tmeaning = meaning_modify_OG[count]\n",
    "                        if tmeaning [0:2] == 'a ': \n",
    "                            tmeaning = tmeaning[2:]\n",
    "                        clean_array.append(\"\\\"\" + tmeaning + \"\\\"\")\n",
    "                        count = count+1\n",
    "                        clean_bool = True\n",
    "                #check for two word terms from term_info\n",
    "                if e < len(message_array)-2 and clean_bool == False: \n",
    "                    t_string = message_array[e] + ' ' + message_array[e+1]\n",
    "                    if t_string.replace('.', '').replace('\\\"', '') in set(term_info['term']): \n",
    "                        tmeaning = meaning_modify_OG[count]\n",
    "                        if tmeaning [0:2] == 'a ': \n",
    "                            tmeaning = tmeaning[2:]\n",
    "                        clean_array.append(\"\\\"\" + meaning_modify_OG[count] + \"\\\"\")\n",
    "                        count = count+1\n",
    "                    else: \n",
    "                        clean_array.append(message_array[e])\n",
    "                #if this part of message_array isn't tagged add the original message part\n",
    "                elif clean_bool == False: \n",
    "                    clean_array.append(message_array[e])\n",
    "            \n",
    "            #add spaces to clean_message created above to make it readable\n",
    "            clean_message = ' '.join([str(elem) for elem in clean_array]) + '.'\n",
    "            \n",
    "            rwarning = 'WARNING: Parts of this message have been censored due to hate speech.'\n",
    "            add_results(results, label, message_OG, clean_message, rwarning, message_vote_ratio, message_class)\n",
    "            print(clean_message) \n",
    "            \n",
    "            \n",
    "        if label >= 3: \n",
    "            print('This message has been blocked due to hate speech about', meaning_string) \n",
    "            rwarning = 'This message has been blocked due to hate speech about ' + meaning_string\n",
    "            add_results(results, label, message_OG, clean_message, rwarning, message_vote_ratio, message_class)\n",
    "            \n",
    "            \n",
    "        print('Client Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.classOG.value_counts()\n",
    "#Class 0 = Hate Speech, Class 1 = Offensive Language, Class 2 = Neither"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.hate_ratio.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(results.label,results.hate_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('full_results_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
