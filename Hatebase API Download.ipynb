{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Downloading and Recording the Hatebase.org Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import json \n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is my personal key to access the API\n",
    "key = {\n",
    "    \"api_key\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication Handshake \n",
    "\n",
    "This simply tells the API that you are about to use it, and gives you access. \n",
    "Base URL: https://api.hatebase.org/1-0/authenticate\n",
    "\n",
    "\"1-0\" represents the version number, and \"authenticate\" represents the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_response = requests.post(\"https://api.hatebase.org/4-4/authenticate\", data=key)\n",
    "#print(auth_response.json()['errors']) #in the event of errors uncomment this line to read them\n",
    "\n",
    "#parameters used for the return.  \n",
    "parameters = {\n",
    "    \"token\": auth_response.json()['result']['token'] \n",
    "}\n",
    "\n",
    "print(\"Status Code: \", auth_response.status_code, \"Token: \", parameters[\"token\"]) #200 means all good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = 1\n",
    "\n",
    "parameters[\"format\"] = \"json\"\n",
    "parameters[\"language\"] = \"ENG\"\n",
    "parameters[\"page\"] = page_count \n",
    "\n",
    "#Initialize dictionaries to store results in until they can be put to files\n",
    "unambiguous = {\n",
    "}\n",
    "\n",
    "ambiguous = {\n",
    "}\n",
    "\n",
    "total = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"https://api.hatebase.org/4-4/get_vocabulary\", data=parameters) #used to retrieve max page number for full API\n",
    "pages_number = response.json()['number_of_pages'] #to be used later to limit the full loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request Loop for Total Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = 1 #make sure page_count has been set to 1 in case other loops have been run. \n",
    "parameters[\"page\"] = page_count \n",
    "\n",
    "for p in range(1, pages_number+1): \n",
    "    response = requests.post(\"https://api.hatebase.org/4-4/get_vocabulary\", data=parameters)\n",
    "    print(\"Status Code: \", response.status_code, \"Page: \", response.json()['page']) #200 means all good\n",
    "    \n",
    "    response_result = response.json()['result'] #retrieve only the results, not the version number, etc\n",
    "    \n",
    "    for i in response_result: #for each result from the current page\n",
    "        total.update( {i['term'] : i}) #add term to total\n",
    "    \n",
    "    page_count += 1\n",
    "    parameters[\"page\"] = page_count #increase and update page_count to retrieve the next page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record Results\n",
    "\n",
    "Record result as in JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_file_json = \"total_results.json\"\n",
    "t_file_csv = \"total_results.csv\"\n",
    "t_file_T_csv = \"total_results_T.csv\"\n",
    "\n",
    "# Writing JSON data to file\n",
    "with open(t_file_json, 'w') as f:\n",
    "    json.dump(total, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(t_file_json).to_csv(t_file_csv) #read in json file to create a csv file from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(t_file_csv, header=None).T.to_csv(t_file_T_csv, header=False, index=False)  #transpose csv so that each term is a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_t=pd.read_csv(t_file_T_csv)\n",
    "\n",
    "keep_col = ['term', 'vocabulary_id', 'hateful_meaning', 'is_unambiguous', 'is_unambiguous_in', 'average_offensiveness', 'plural_of', 'variant_of', \\\n",
    "            'is_about_nationality', 'is_about_ethnicity', 'is_about_religion', 'is_about_gender', 'is_about_sexual_orientation', \\\n",
    "            'is_about_disability', 'is_about_class', 'number_of_sightings']\n",
    "\n",
    "new_temp_t = temp_t[keep_col] #keep only relevant columns, not date spotted etc\n",
    "new_temp_t.to_csv(\"total_results_T_trim.csv\", index=False) #create final csv for full API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Ambiguous Split\n",
    "\n",
    "Split the total_results_T_trim.csv into total_unambiguous.csv and total_ambiguous.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = pd.read_csv('total_results_T_trim.csv')\n",
    "\n",
    "force_arr = ['niger', 'nigger', 'nigers', 'niggers'] #create array of terms you want to force to be unambiguous\n",
    "\n",
    "force_index_arr = total_results.index[total_results['term'].isin(force_arr)].tolist()\n",
    "force_index_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in force_index_arr:\n",
    "    total_results.at[element, 'is_unambiguous'] = True #make all terms previously stated forced to unambiguous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unambiguous_results = total_results[total_results.is_unambiguous == True]\n",
    "ambiguous_results = total_results[total_results.is_unambiguous == False] \n",
    "print(\"Total Shape: \", total_results.shape, \"Unambiguous Shape: \", unambiguous_results.shape, \\\n",
    "      \"Ambiguous Shape: \", ambiguous_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unambiguous_results.to_csv(\"total_unambiguous_results.csv\", index=False)\n",
    "ambiguous_results.to_csv(\"total_ambiguous_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim NoSwearing Data\n",
    "Remove words already in the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noswearing_DB = pd.read_csv('noswearing_original_data.csv')\n",
    "hatebase_total_DB = pd.read_csv('total_results_T_trim.csv')\n",
    "noswearing_list = noswearing_DB['term']\n",
    "hatebase_total_list = hatebase_total_DB['term']\n",
    "len(noswearing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noswearing_trim_list = list(set(noswearing_list) - set(hatebase_total_list)) #remove any duplicates already present in the Hatebase Database\n",
    "len(noswearing_trim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noswearing_final = noswearing_DB[noswearing_DB['term'].isin(noswearing_trim_list)] #only keep rows that weren't trimmed\n",
    "noswearing_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noswearing_final.to_csv('noswearing_trim_data.csv', index=False) #create final noswearing list to work as amplifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
